{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from official.nlp import bert\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.tokenization\n",
    "\n",
    "from keras_bert.loader import load_trained_model_from_checkpoint\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "from squad_test import compute_f1, compute_exact\n",
    "\n",
    "from bert_test import Bert\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = \"bert/model/pretrained\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_LIMIT = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHKPT_SAVE_DIR = 'bert/chkpt/save/dir/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizer to generate Tensorflow dataset\n",
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(BERT_PRETRAINED_DIR, \"vocab.txt\"),\n",
    "     do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pretrained = load_trained_model_from_checkpoint(config_file, checkpoint_file, \n",
    "                                                     training=True, seq_len=TOKEN_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dense layer to end of Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_encoder_layer = -7\n",
    "\n",
    "sequence_output = bert_pretrained.layers[last_encoder_layer].output\n",
    "\n",
    "pool_output = Dense(2, kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                    name = 'real_output')(sequence_output)\n",
    "\n",
    "bert_model = Model(inputs=bert_pretrained.input, outputs=pool_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay = 0.01 # same as used in BERT paper\n",
    "\n",
    "adam = Adam(lr=learning_rate, decay=decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax CrossEntropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSquadError(tf.keras.losses.Loss):\n",
    "\n",
    "    '''\n",
    "        positions: tensor of size batch_size x 2; [answer_start_index, answer_end_index]\n",
    "        logits: tensor of size batch_size x max_tokens x 2\n",
    "    '''\n",
    "    def call(self, positions, logits):\n",
    "        \n",
    "        logits = tf.transpose(logits, [0, 2, 1])\n",
    "    \n",
    "        # logits' shape is [2, squence_length]\n",
    "        start_logits = logits[:, 0]\n",
    "        end_logits = logits[:, 1]\n",
    "\n",
    "        one_hot_positions = tf.one_hot(\n",
    "            positions, depth=logits.shape[2], dtype=tf.float32)\n",
    "        # one_hot_positions' shape is [2, squence_length]\n",
    "        start_positions = one_hot_positions[:, 0]\n",
    "        end_positions = one_hot_positions[:, 1]\n",
    "\n",
    "        log_probs = tf.nn.log_softmax(start_logits, axis=-1)\n",
    "        loss_start = -tf.reduce_mean(tf.reduce_sum(start_positions * log_probs, axis=-1),axis=-1)\n",
    "\n",
    "        log_probs = tf.nn.log_softmax(end_logits, axis=-1)\n",
    "        loss_end = -tf.reduce_mean(tf.reduce_sum(end_positions * log_probs, axis=-1),axis=-1)\n",
    "\n",
    "        loss_total = tf.reduce_mean([loss_start, loss_end])\n",
    "\n",
    "        return loss_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.compile(loss=BertSquadError(), optimizer=adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('squad_feats.npy')\n",
    "labels = np.load('squad_labs.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_data = tf.data.Dataset.from_tensor_slices(({\"Input-Token\": train[0],\n",
    "                                   \"Input-Segment\": train[1],\n",
    "                                   \"Input-Masked\": train[2]},\n",
    "                                  labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_data = squad_train_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_model.fit(squad_train_data, verbose=1, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_weights(CHKPT_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(bert, collection, docs, v2=True, max_iter=None):\n",
    "\n",
    "    contents = []\n",
    "    questions = []\n",
    "    new_col = []\n",
    "\n",
    "    for j in range(len(collection)):\n",
    "        if collection[j]['label'] and not v2:\n",
    "            continue\n",
    "\n",
    "        questions.append(collection[j]['question'])\n",
    "        contents.append(docs[collection[j]['id']])\n",
    "        new_col.append(collection[j])\n",
    "        \n",
    "    collection = new_col\n",
    "    \n",
    "    if max_iter is None:\n",
    "        max_iter = len(collection)\n",
    "\n",
    "    data, indices = bert.encode_contents(questions, contents)\n",
    "\n",
    "    bert_results = bert.predict(questions, contents, data, indices)\n",
    "\n",
    "    em_pred = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for j in range(max_iter):\n",
    "\n",
    "        answers = collection[j]['answers']\n",
    "\n",
    "        results = bert_results[j]\n",
    "\n",
    "        bert_ans = results[0]\n",
    "\n",
    "        best_null = results[1] < results[2]\n",
    "\n",
    "        if best_null and v2:\n",
    "            em_pred.append(float(collection[j]['label']))\n",
    "            f1_scores.append(float(collection[j]['label']))\n",
    "        else:\n",
    "            match = 0\n",
    "            f1_s = 0\n",
    "            for answer in answers:\n",
    "                match = match or compute_exact(answer, bert_ans)\n",
    "                new_f1, new_recall = compute_f1(answer, bert_ans)\n",
    "                f1_s = max(f1_s, new_f1)\n",
    "\n",
    "            em_pred.append(match)\n",
    "            f1_scores.append(f1_s)\n",
    "\n",
    "    return em_pred, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = Bert(TOKEN_LIMIT, BERT_PRETRAINED_DIR, CHKPT_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('squad_val_questions.pkl', 'rb') as input:\n",
    "    questions = pkl.load(input)\n",
    "    \n",
    "with open('squad_val_contexts.pkl', 'rb') as input:\n",
    "    contexts = pkl.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test With Impossible Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_pred, f1_scores = model_validation(bert, questions[:10], contexts, v2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The v2.0 EM score for this model is: {}%'.format(round(np.mean(em_pred), 4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The v2.0 F1 score for this model is: {}%'.format(round(np.mean(em_pred), 4) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Without Impossible Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_pred, f1_scores = model_validation(bert, questions[:10], contexts, v2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The v1.1 EM score for this model is: {}%'.format(round(np.mean(em_pred), 4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The v1.1 F1 score for this model is: {}%'.format(round(np.mean(em_pred), 4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goggles",
   "language": "python",
   "name": "goggles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
