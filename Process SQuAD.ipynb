{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.bert.configs\n",
    "\n",
    "from squad_preprocess import convert_squad_to_features\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_LIMIT = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_DIR = \"bert/pretrained/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(BERT_PRETRAINED_DIR, \"vocab.txt\"),\n",
    "     do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SQuAD v2.0 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = tfds.question_answering.squad.Squad()\n",
    "sq = tfds.question_answering.squad.Squad(config=sq.BUILDER_CONFIGS[1])\n",
    "squad_v2 = sq.as_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_examples(squad_dataset, max_iter=None):\n",
    "\n",
    "    Example = collections.namedtuple(\"Example\", [\"doc_tokens\", \"is_impossible\",\n",
    "                                 'start_position', 'end_position', \n",
    "                                 'orig_answer_text', 'question_text'])\n",
    "    \n",
    "    if max_iter is None:\n",
    "        max_iter = len(squad_dataset)\n",
    "    \n",
    "    examples = []\n",
    "\n",
    "    for i, s in enumerate(squad_dataset.as_numpy_iterator()):\n",
    "        \n",
    "        if i == max_iter:\n",
    "            break\n",
    "\n",
    "        question_text = s['question']\n",
    "        is_impossible = ['is_impossible']\n",
    "        context = s['context'].decode(\"utf-8\").lower()\n",
    "\n",
    "        doc_tokens = re.split('[\\s]+', context)\n",
    "        if is_impossible:\n",
    "            start_position = -1\n",
    "            end_position = -1\n",
    "            orig_answer_text = ''\n",
    "        else:\n",
    "            orig_answer_text = s['answers']['text'][0]\n",
    "\n",
    "            delimiter = '<HERE!>'\n",
    "            start_index = s['answers']['answer_start'][0]\n",
    "            text = s['context'].decode(\"utf-8\").lower()\n",
    "            text = text[:start_index] + delimiter + text[start_index:]\n",
    "            words = re.split('[\\s]+', text)\n",
    "            for word_index, word in enumerate(words):\n",
    "                if delimiter in word:\n",
    "                    start_position = word_index\n",
    "                    answer_length = len(re.split('[\\s]+', orig_answer_text.decode(\"utf-8\").lower()))\n",
    "                    end_position = word_index + answer_length\n",
    "\n",
    "        example = Example(is_impossible=is_impossible, doc_tokens=doc_tokens, \n",
    "                           orig_answer_text=orig_answer_text, start_position=start_position, \n",
    "                           end_position=end_position, question_text=question_text)\n",
    "\n",
    "        examples.append(example)\n",
    "\n",
    "        # shows progress\n",
    "        if (len(examples) + 1) % 500 == 0:\n",
    "                from IPython.display import clear_output\n",
    "                clear_output(wait=True)\n",
    "                print('{0:.2f}%'.format(round(len(examples) / max_iter, 4) * 100))\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = squad_v2['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = extract_examples(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = convert_squad_to_features(examples, tokenizer, WORD_LIMIT, 300, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('squad_features_{}_{}.pkl'.format(WORD_LIMIT, BERT_PRETRAINED_DIR.split('_')[-1]), 'wb') as input:\n",
    "    pickle.dump(features, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_ids = []\n",
    "input_masks = []\n",
    "input_types = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "for feature in features:\n",
    "    \n",
    "    input_ids.append(feature['input_ids'])\n",
    "    input_masks.append(feature['input_mask'])\n",
    "    input_types.append(feature['segment_ids'])\n",
    "    \n",
    "    labels.append([feature['start_position'], feature['end_position']])\n",
    "    \n",
    "squad_f = np.asarray([input_ids, input_types, input_masks])\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('squad_feats_{}_{}'.format(WORD_LIMIT, BERT_PRETRAINED_DIR.split('_')[-1]), squad_f)\n",
    "np.save('squad_labs_{}_{}'.format(WORD_LIMIT, BERT_PRETRAINED_DIR.split('_')[-1]), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_id(content, docs):\n",
    "    keys_list = list(docs.keys())\n",
    "    vals_list = list(docs.values())\n",
    "\n",
    "    key_val_index = vals_list.index(content)\n",
    "\n",
    "    return keys_list[key_val_index]\n",
    "\n",
    "def bin_to_str(binary_tf):\n",
    "\n",
    "    binary_tf = binary_tf.numpy()\n",
    "\n",
    "    if type(binary_tf) is bytes:\n",
    "        return binary_tf.decode(\"utf-8\")\n",
    "    else:\n",
    "        result = []\n",
    "        for bin_tf in binary_tf:\n",
    "            result.append(bin_tf.decode(\"utf-8\"))\n",
    "\n",
    "        return result\n",
    "\n",
    "def questions_and_contexts(dataset):\n",
    "    docs = {}\n",
    "    collection = []\n",
    "    already_in = []\n",
    "\n",
    "    for doc_id, doc in enumerate(dataset):\n",
    "        content = None\n",
    "        current_id = doc_id\n",
    "        ques_and_ans = {}\n",
    "\n",
    "        content = bin_to_str(doc['context'])\n",
    "        if content in already_in:\n",
    "            current_id = get_doc_id(content, docs)\n",
    "        else:\n",
    "            already_in.append(content)\n",
    "            docs[doc_id] = content\n",
    "\n",
    "        ques_and_ans['question'] = bin_to_str(doc['question'])\n",
    "        ques_and_ans['id'] = current_id\n",
    "        ques_and_ans['answers'] = bin_to_str(doc['answers']['text'])\n",
    "\n",
    "        ques_and_ans['label'] = doc['is_impossible'].numpy()\n",
    "        ques_and_ans['plaus_answers'] = bin_to_str(\n",
    "            doc['plausible_answers']['text'])\n",
    "\n",
    "        collection.append(ques_and_ans)\n",
    "        \n",
    "        # shows progress\n",
    "        if (doc_id + 1) % 500 == 0:\n",
    "                from IPython.display import clear_output\n",
    "                clear_output(wait=True)\n",
    "                print('{0:.2f}%'.format(round(doc_id / len(dataset), 4) * 100))\n",
    "\n",
    "    return collection, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = squad_v2['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_questions, val_contexts = questions_and_contexts(val_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('squad_val_questions.pkl', 'wb') as input:\n",
    "    pickle.dump(val_questions, input)\n",
    "    \n",
    "with open('squad_val_contexts.pkl', 'wb') as input:\n",
    "    pickle.dump(val_contexts, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucene Index JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_to_str(binary_tf):\n",
    "\n",
    "    binary_tf = binary_tf.numpy()\n",
    "\n",
    "    if type(binary_tf) is bytes:\n",
    "        return binary_tf.decode(\"utf-8\")\n",
    "    else:\n",
    "        result = []\n",
    "        for bin_tf in binary_tf:\n",
    "            result.append(bin_tf.decode(\"utf-8\"))\n",
    "\n",
    "        return result\n",
    "\n",
    "def collect_contexts(dataset, start_docId):\n",
    "    docs = {}\n",
    "    all_contexts = set()\n",
    "\n",
    "    for doc_id, doc in enumerate(dataset):\n",
    "        content = None\n",
    "        current_id = doc_id + start_docId\n",
    "        \n",
    "        content = bin_to_str(doc['context'])\n",
    "        \n",
    "        if content not in all_contexts:\n",
    "            all_contexts.add(content)\n",
    "            docs[doc_id] = content\n",
    "        \n",
    "        # shows progress\n",
    "        if (doc_id + 1) % 500 == 0:\n",
    "                from IPython.display import clear_output\n",
    "                clear_output(wait=True)\n",
    "                print('{0:.2f}%'.format(round(doc_id / len(dataset), 4) * 100))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a document contains a docId and content\n",
    "all_documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Validation Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docId, content in val_contexts.items():\n",
    "    docum = {}\n",
    "    docum['id'] = docId\n",
    "    docum['contents'] = content\n",
    "    \n",
    "    all_documents.append(docum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_last_docId = max(val_contexts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Train Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts = collect_contexts(train_samples, val_last_docId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docId, content in train_contexts.items():\n",
    "    docum = {}\n",
    "    docum['id'] = docId\n",
    "    docum['contents'] = content\n",
    "    \n",
    "    all_documents.append(docum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = json.encoder.JSONEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = encoder.encode(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.json', 'w') as file:\n",
    "    file.writelines(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To create Index\n",
    "\n",
    "Run the command below in the terminal, replacing Index_dir with where the Index will be stored and JSON_dir with the directory the json file is in\n",
    "\n",
    "python3 -m pyserini.index -collection JsonCollection -generator DefaultLuceneDocumentGenerator  -threads 2 -input JSON_dir -index Index_dir -storePositions -storeDocvectors -storeRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goggles",
   "language": "python",
   "name": "goggles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
